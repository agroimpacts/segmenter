{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import gdal\n",
    "import yaml\n",
    "from yaml import Loader\n",
    "from shapely.geometry import mapping\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from osgeo import gdal_array\n",
    "from skimage.filters import sobel\n",
    "from sklearn import preprocessing\n",
    "from skimage.segmentation import slic, watershed\n",
    "from osgeo import ogr\n",
    "import cv2 as cv\n",
    "from skimage.future import graph\n",
    "import subprocess\n",
    "from osgeo import osr\n",
    "from sklearn import preprocessing\n",
    "# from skimage.color import rgb2gra\n",
    "\n",
    "# import shapely.geometry import mapping\n",
    "\n",
    "def parse_catalog_from_s3(bucket, prefix, catalog_name):\n",
    "    \"\"\"\n",
    "    read bucket, prefix from yaml.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: prefix for yaml file\n",
    "        catalog_name: name of catalog file\n",
    "    return:\n",
    "        'catalog' pandas object\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    obj = s3.get_object(Bucket=bucket, Key='{}/{}'.format(prefix, catalog_name))\n",
    "    catalog = pd.read_csv(obj['Body'], sep=\" \")\n",
    "    return catalog\n",
    "\n",
    "def parse_yaml_from_s3(bucket, prefix):\n",
    "    \"\"\"\n",
    "    read bucket, prefix from yaml.\n",
    "    arg:\n",
    "        bucket: Name of the S3 bucket.\n",
    "        prefix: the name for yaml file\n",
    "    return:\n",
    "        yaml object\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Bucket(bucket).Object(prefix).get()['Body'].read()\n",
    "    return yaml.load(obj)\n",
    "\n",
    "def get_colrow_geojson(foc_gpd_tile, left_corner_x, left_corner_y, per_tile_width, logger):\n",
    "    extent_geojson = mapping(foc_gpd_tile['geometry'])\n",
    "    center_x = (extent_geojson['bbox'][0] + extent_geojson['bbox'][2]) / 2\n",
    "    center_y = (extent_geojson['bbox'][1] + extent_geojson['bbox'][3]) / 2\n",
    "    row = int(round((left_corner_y - center_y +  per_tile_width / 2) / per_tile_width)) - 1\n",
    "    col = int(round((center_x - left_corner_x + per_tile_width / 2) / per_tile_width)) - 1\n",
    "    return (col, row)\n",
    "\n",
    "def weight_mean_color(graph, src, dst, n):\n",
    "    \"\"\"Callback to handle merging nodes by recomputing mean color.\n",
    "\n",
    "    The method expects that the mean color of `dst` is already computed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : RAG\n",
    "        The graph under consideration.\n",
    "    src, dst : int\n",
    "        The vertices in `graph` to be merged.\n",
    "    n : int\n",
    "        A neighbor of `src` or `dst` or both.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        A dictionary with the `\"weight\"` attribute set as the absolute\n",
    "        difference of the mean color between node `dst` and `n`.\n",
    "    \"\"\"\n",
    "\n",
    "    diff = graph.node[dst]['mean color'] - graph.node[n]['mean color']\n",
    "    diff = np.linalg.norm(diff)\n",
    "    return {'weight': diff}\n",
    "\n",
    "\n",
    "def merge_mean_color(graph, src, dst):\n",
    "    \"\"\"Callback called before merging two nodes of a mean color distance graph.\n",
    "\n",
    "    This method computes the mean color of `dst`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : RAG\n",
    "        The graph under consideration.\n",
    "    src, dst : int\n",
    "        The vertices in `graph` to be merged.\n",
    "    \"\"\"\n",
    "    graph.node[dst]['total color'] += graph.node[src]['total color']\n",
    "    graph.node[dst]['pixel count'] += graph.node[src]['pixel count']\n",
    "    graph.node[dst]['mean color'] = (graph.node[dst]['total color'] /\n",
    "                                     graph.node[dst]['pixel count'])\n",
    "\n",
    "def gdal_save_file_tif_3bands(outpath, r, g, b, gdal_type, trans, proj, rows, cols):\n",
    "    \"\"\"\n",
    "    save file\n",
    "    Parameters\n",
    "    ----------\n",
    "    outpath : full outputted path\n",
    "    r : the first band (numpy array)\n",
    "    g : the second band (numpy array)\n",
    "    b : the third band (numpy array)\n",
    "    gdal_type: gdal type\n",
    "    trans: transform coefficients\n",
    "    proj: projection\n",
    "    Returns\n",
    "    -------\n",
    "    TRUE OR FALSE\n",
    "    \"\"\"\n",
    "    outdriver = gdal.GetDriverByName(\"GTiff\")\n",
    "    outdata = outdriver.Create(outpath, rows, cols, 3, gdal_type)\n",
    "    if outdata is None:\n",
    "        return False\n",
    "    outdata.GetRasterBand(1).WriteArray(r)\n",
    "    outdata.FlushCache()\n",
    "    outdata.GetRasterBand(2).WriteArray(g)\n",
    "    outdata.FlushCache()\n",
    "    outdata.GetRasterBand(3).WriteArray(b)\n",
    "    outdata.FlushCache()\n",
    "    outdata.SetGeoTransform(trans)\n",
    "    outdata.FlushCache()\n",
    "    outdata.SetProjection(proj)\n",
    "    outdata.FlushCache()\n",
    "    outdata = None\n",
    "    return True\n",
    "\n",
    "\n",
    "def gdal_save_file_tif_1bands(outpath, array, gdal_type, trans, proj, rows, cols):\n",
    "    \"\"\"\n",
    "    save file\n",
    "    Parameters\n",
    "    ----------\n",
    "    outpath : full outputted path\n",
    "    array : numpy array to be saved\n",
    "    gdal_type: gdal type\n",
    "    trans: transform coefficients\n",
    "    proj: projection\n",
    "    Returns\n",
    "    -------\n",
    "    TRUE OR FALSE\n",
    "    \"\"\"\n",
    "    outdriver = gdal.GetDriverByName(\"GTiff\")\n",
    "    outdata = outdriver.Create(outpath, rows, cols, 3, gdal_type)\n",
    "    if outdata is None:\n",
    "        return False\n",
    "    outdata.GetRasterBand(1).WriteArray(array)\n",
    "    outdata.FlushCache()\n",
    "    outdata.SetGeoTransform(trans)\n",
    "    outdata.FlushCache()\n",
    "    outdata.SetProjection(proj)\n",
    "    outdata.FlushCache()\n",
    "    outdata = None\n",
    "    return True\n",
    "\n",
    "tile_id = str(486641)\n",
    "# 486255\n",
    "# 486650\n",
    "# 486641\n",
    "season = 'GS'\n",
    "tmp_pth = '/tmp'\n",
    "buf = 11\n",
    "s3_bucket = 'activemapper'\n",
    "aoi = '1'\n",
    "left_corner_x = -17.541\n",
    "left_corner_y = 37.54\n",
    "per_tile_width = 0.005 * 10 # 0.005 degree is the width of cells, 1 tile has 10*10 cells\n",
    "\n",
    "\n",
    "log_path = '%s/log/segmenter_%s.log' % (os.environ['HOME'], str(aoi))\n",
    "logging.basicConfig(filename=log_path, filemode='w', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "with open(\"/home/ubuntu/source/segmenter_config.yaml\", 'r') as yaml_obj:\n",
    "    params = yaml.safe_load(yaml_obj)['segmenter']\n",
    "    \n",
    "prefix = params['planet_prefix']\n",
    "planet_directory = params['planet_directory']\n",
    "prob_directory = params['prob_directory']\n",
    "tiles_geojson_path = params['tile_geojson_path']\n",
    "mmu = params['mmu']\n",
    "prob_threshold = params['prob_threshold']\n",
    "dry_lower_ordinal = params['dry_lower_ordinal']  # 2018/12/01\n",
    "dry_upper_ordinal = params['dry_upper_ordinal']  # 2019/02/28\n",
    "wet_lower_ordinal = params['wet_lower_ordinal']  # 2018/05/01\n",
    "wet_upper_ordinal = params['wet_upper_ordinal']  # 2018/09/30\n",
    "\n",
    "working_dir  = '/tmp'\n",
    "verbose = False\n",
    "buf = 11 # we have a buf for each 2000 * 2000 tile\n",
    "proj = ''\n",
    "\n",
    "uri_tile = \"s3://{}/{}/{}\".format(s3_bucket, prefix, tiles_geojson_path)\n",
    "gpd_tile = gpd.read_file(uri_tile)\n",
    "if gpd_tile is None:\n",
    "    logger.error(\"reading geojson tile '{}' failed\". format(uri_tile))\n",
    "foc_gpd_tile = gpd_tile[gpd_tile['tile'] == int(tile_id)]\n",
    "(tile_col, tile_row) = get_colrow_geojson(foc_gpd_tile, left_corner_x, left_corner_y, per_tile_width, logger)\n",
    "if season is 'OS':\n",
    "    uri_composite_gdal = \"/vsis3/{}/{}/{}/tile{}_{}_{}.tif\".format(s3_bucket, planet_directory, season, tile_id, dry_lower_ordinal, dry_upper_ordinal)\n",
    "else:\n",
    "    uri_composite_gdal = \"/vsis3/{}/{}/{}/tile{}_{}_{}.tif\".format(s3_bucket, planet_directory, season, tile_id, wet_lower_ordinal, wet_upper_ordinal)\n",
    "uri_prob_gdal = \"/vsis3/{}/{}/image_c{}_r{}.tif\".format(s3_bucket, prob_directory, str(tile_col), str(tile_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/vsis3/activemapper/classified-images/1_whole_test/image_c300_r532.tif'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri_prob_gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick way to read image as numpy array\n",
    "array_composite = gdal_array.LoadFile(uri_composite_gdal)\n",
    "array_prob = gdal_array.LoadFile(uri_prob_gdal)\n",
    "\n",
    "# get channels, cols, rows\n",
    "[nchannels, cols, rows] = array_composite.shape\n",
    "\n",
    "# temporal change for dealing with Ron's Rf probability image\n",
    "# array_prob = array_prob[buf:cols - buf, buf:rows - buf]\n",
    "\n",
    "# STEP 1\n",
    "# meanshift algorithm to smooth image and filter out noise\n",
    "B1, b2, b3, b4 = array_composite\n",
    "\n",
    "# scale to int8 for opencv processing\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 255))\n",
    "\n",
    "# get min and max\n",
    "b2_scale = min_max_scaler.fit_transform(b2.astype(np.float64).reshape(cols * rows, 1)).astype(np.uint8)\n",
    "b3_scale = min_max_scaler.fit_transform(b3.astype(np.float64).reshape(cols * rows, 1)).astype(np.uint8)\n",
    "b4_scale = min_max_scaler.fit_transform(b4.astype(np.float64).reshape(cols * rows, 1)).astype(np.uint8)\n",
    "\n",
    "# keep records for min and max for original bands\n",
    "b2_min = np.min(b2)\n",
    "b2_max = np.max(b2)\n",
    "b3_min = np.min(b3)\n",
    "b3_max = np.max(b3)\n",
    "b4_min = np.min(b4)\n",
    "b4_max = np.max(b4)\n",
    "\n",
    "mat_norm = cv.merge([b2_scale.reshape(cols, rows), b3_scale.reshape(cols, rows), b4_scale.reshape(cols, rows)])\n",
    "# meanshift algorithm\n",
    "dst_norm = cv.pyrMeanShiftFiltering(mat_norm, 5, 5, termcrit=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 1, 5))\n",
    "\n",
    "# rescale to int16 (i.e., original band data range)\n",
    "b2_m, b3_m, b4_m = cv.split(dst_norm)\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(b2_min, b2_max))\n",
    "b2_filter = min_max_scaler.fit_transform(b2_m.reshape(cols * rows, 1).astype(np.float32))\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(b3_min, b3_max))\n",
    "b3_filter = min_max_scaler.fit_transform(b3_m.reshape(cols * rows, 1).astype(np.float32))\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(b4_min, b4_max))\n",
    "b4_filter = min_max_scaler.fit_transform(b4_m.reshape(cols * rows, 1).astype(np.float32))\n",
    "\n",
    "# mat_filter_rescale = cv.merge([b2_filter.reshape(cols, rows),\n",
    "#                    b3_filter.reshape(cols, rows),\n",
    "#                    b4_filter.reshape(cols, rows)])\n",
    "# r, g, b = cv.split(mat_filter_rescale)\n",
    "\n",
    "array_filter = np.dstack((b2_filter.reshape(cols, rows),\n",
    "                                b3_filter.reshape(cols, rows),\n",
    "                                b4_filter.reshape(cols, rows)))\n",
    "\n",
    "# save the results for checking\n",
    "# read metadata info\n",
    "if verbose is True:\n",
    "    metadata = gdal.Open(tile_path)\n",
    "    trans = metadata.GetGeoTransform()\n",
    "    # proj = metadata.GetProjection()\n",
    "    outpath = os.path.join(working_dir, 'tile{}_{}_meanshift.tif'.format(tile_id, season))\n",
    "    gdal_save_file_tif_3bands(outpath, b2_filter.reshape(cols, rows),\n",
    "                              b3_filter.reshape(cols, rows),\n",
    "                              b4_filter.reshape(cols, rows),\n",
    "                              gdal.GDT_Float32, trans, proj, rows, cols)\n",
    "\n",
    "\n",
    "# STEP 2\n",
    "# sober filtering + watershed\n",
    "r_sobel = sobel(b2_filter.reshape(cols, rows))\n",
    "g_sobel = sobel(b3_filter.reshape(cols, rows))\n",
    "b_sobel = sobel(b4_filter.reshape(cols, rows))\n",
    "gradient = r_sobel + g_sobel + b_sobel\n",
    "gradient_subset = gradient[buf:cols - buf, buf:rows - buf]\n",
    "# peak_gradient = peak_local_max(-gradient, num_peaks=2400, min_distance=10, indices=False)\n",
    "# markers = ndi.label(peak_gradient)[0]\n",
    "\n",
    "# For a 2D image, a connectivity of 1 corresponds to immediate neighbors up, down, left, and right,\n",
    "# while a connectivity of 2 also includes diagonal neighbors.\n",
    "segments_watershed = watershed(gradient_subset, markers=2400, connectivity=2, compactness=0).astype(np.int16)\n",
    "\n",
    "# read metadata info\n",
    "metadata = gdal.Open(uri_prob_gdal)\n",
    "trans = metadata.GetGeoTransform()\n",
    "# proj = metadata.GetProjection()\n",
    "\n",
    "if verbose is True:\n",
    "    outpath = os.path.join(working_dir, 'tile{}_{}_meanshift_sober.tif'.format(tile_id, season))\n",
    "    gdal_save_file_tif_1bands(outpath, gradient_subset, gdal.GDT_Float32, trans, proj, rows - 2 * buf, cols - 2 * buf)\n",
    "    outpath = os.path.join(working_dir, 'tile{}_{}_meanshift_watershed.tif'.format(tile_id, season))\n",
    "    gdal_save_file_tif_1bands(outpath, segments_watershed, gdal.GDT_Int16, trans, proj, rows - 2 * buf, cols - 2 * buf)\n",
    "\n",
    "# segments_slic = slic(arr_filter_rescale, n_segments=2400,compactness=0.1)\n",
    "# outpath = os.path.join(working_path, 'split_tile1_os_slic.tif')\n",
    "# gdal_save_file_tif_1bands(outpath, segments_slic, gdal.GDT_Int16, trans, proj, rows, cols)\n",
    "\n",
    "\n",
    "# STEP 3\n",
    "# overlapped with prob image, and selected those polygons that have average probability over 0.5\n",
    "# segments_watershed_merge_sieve = gdal_array.LoadFile(os.path.join(working_dir, tile_id + '_watershed_merge_sieve.tif'))\n",
    "for i in range(1, np.max(segments_watershed)):\n",
    "    condition = np.equal(segments_watershed, i)\n",
    "    prob_condition = np.extract(condition, array_prob)\n",
    "    if np.mean(prob_condition) < prob_threshold:\n",
    "        segments_watershed[condition] = 0\n",
    "\n",
    "\n",
    "if verbose is True:\n",
    "    outpath = os.path.join(working_dir,  'tile{}_{}_watershed_overlap.tif'.format(tile_id, season))\n",
    "    gdal_save_file_tif_1bands(outpath, segments_watershed, gdal.GDT_Int16, trans, proj, rows - 2 * buf,\n",
    "                              cols - 2 * buf)\n",
    "\n",
    "# STEP 4\n",
    "# connected small polygons using graph theory + sieving filter\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "array_original_rescale = np.dstack((min_max_scaler.fit_transform(b2.reshape(cols * rows, 1).astype(np.float32))\n",
    "                                    .reshape(cols, rows),\n",
    "                                    min_max_scaler.fit_transform(b3.reshape(cols * rows, 1).astype(np.float32))\n",
    "                                    .reshape(cols, rows),\n",
    "                                    min_max_scaler.fit_transform(b4.reshape(cols * rows, 1).astype(np.float32))\n",
    "                                    .reshape(cols, rows)))\n",
    "\n",
    "b2_filter_withingrid = array_original_rescale[buf:cols - buf, buf:rows - buf, 0]\n",
    "field_b2_std = np.std(b2_filter_withingrid[array_prob > prob_threshold])\n",
    "# field_b2_std = np.std(b2_filter_withingrid[arr_prob > prob_threshold])\n",
    "# nofield_b2_mean = np.mean(b2_filter_withingrid[array_prob <= prob_threshold])\n",
    "# nofield_b2_std = np.std(b2_filter_withingrid[arr_prob <= prob_threshold])\n",
    "\n",
    "b3_filter_withingrid = array_original_rescale[buf:cols - buf, buf:rows - buf, 1]\n",
    "field_b3_std = np.std(b3_filter_withingrid[array_prob > prob_threshold])\n",
    "# nofield_b3_mean = np.mean(b3_filter_withingrid[array_prob <= threshold])\n",
    "\n",
    "b4_filter_withingrid = array_original_rescale[buf:cols - buf, buf:rows - buf, 2]\n",
    "field_b4_std = np.std(b4_filter_withingrid[array_prob > prob_threshold])\n",
    "# nofield_b4_mean = np.mean(b4_filter_withingrid[array_prob <= prob_threshold])\n",
    "\n",
    "# calculate norm to adaptively decide merging threshold\n",
    "# diff_norm = np.linalg.norm([nofield_b2_mean - field_b2_mean, nofield_b3_mean - field_b3_mean,\n",
    "#                             nofield_b4_mean - field_b4_mean])\n",
    "diff_norm = np.linalg.norm([field_b2_std, field_b3_std, field_b4_std])\n",
    "\n",
    "array_original_subset = array_original_rescale[buf:cols - buf, buf:rows - buf]\n",
    "\n",
    "# assign -9999 so background pixels won't be merged\n",
    "array_original_subset[segments_watershed == 0] = [-9999, -9999, -9999]\n",
    "\n",
    "g = graph.rag_mean_color(array_original_subset, segments_watershed,\n",
    "                         mode='distance')\n",
    "\n",
    "segments_watershed_merge = graph.merge_hierarchical(segments_watershed, g, thresh=diff_norm/2, rag_copy=False,\n",
    "                                                    in_place_merge=True,\n",
    "                                                    merge_func=merge_mean_color,\n",
    "                                                    weight_func=weight_mean_color)\n",
    "\n",
    "# note after merging, it sometimes reset polygons id, which cause the id of no field to be not zero any more, the below\n",
    "# is the function to fix this issue\n",
    "for i in range(np.max(segments_watershed_merge)):\n",
    "    condition = np.equal(segments_watershed_merge, i)\n",
    "    id_condition = np.extract(condition, segments_watershed)\n",
    "    counts = np.bincount(id_condition)\n",
    "    if np.argmax(counts) == 0:\n",
    "        nofield_id = i\n",
    "        break\n",
    "\n",
    "if nofield_id is not 0:\n",
    "    # for nofield id is not 0, switch 0 and nofield id\n",
    "    segments_watershed_merge[segments_watershed_merge == 0] = 9999\n",
    "    segments_watershed_merge[segments_watershed_merge == nofield_id] = 0\n",
    "    segments_watershed_merge[segments_watershed_merge == 9999] = nofield_id\n",
    "\n",
    "\n",
    "# required to save it for sieve filter\n",
    "outpath = os.path.join(working_dir, 'tile{}_{}_watershed_overlap_merge.tif'.format(tile_id, season))\n",
    "gdal_save_file_tif_1bands(outpath, segments_watershed_merge, gdal.GDT_Int16, trans, proj, rows - 2 * buf,\n",
    "                          cols - 2 * buf)\n",
    "\n",
    "\n",
    "# sieving filter\n",
    "cmd = 'gdal_sieve.py -q -st {} -8 {} {}'.format(mmu, os.path.join(working_dir, 'tile{}_{}_watershed_overlap_merge.tif'.format(tile_id, season)),\n",
    "                                                os.path.join(working_dir, 'tile{}_{}_watershed_overlap_merge_sieve.tif'.format(tile_id, season)))\n",
    "os.system(cmd)\n",
    "\n",
    "# STEP 5\n",
    "# polyganize\n",
    "outpath = os.path.join(working_dir, 'tile{}_{}_watershed_overlap_merge_sieve.tif'.format(tile_id, season))\n",
    "src_ds = gdal.Open(outpath)\n",
    "if src_ds is None:\n",
    "    print('Unable to open %s' % outpath)\n",
    "\n",
    "srcband = src_ds.GetRasterBand(1)\n",
    "\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromWkt(proj)\n",
    "dst_layername = os.path.join(working_dir, 'tile{}_{}_watershed_merge_sieve_overlap'.format(tile_id, season))\n",
    "drv = ogr.GetDriverByName(\"GeoJSON\")\n",
    "dst_ds = drv.CreateDataSource(dst_layername + \".geojson\")\n",
    "dst_layer = dst_ds.CreateLayer(dst_layername, srs = srs)\n",
    "\n",
    "fieldName = \"id\"\n",
    "fd = ogr.FieldDefn(fieldName, ogr.OFTInteger)\n",
    "dst_layer.CreateField(fd)\n",
    "dst_field = dst_layer.GetLayerDefn().GetFieldIndex(\"id\")\n",
    "\n",
    "gdal.Polygonize(srcband, None, dst_layer, dst_field, [], callback=None)\n",
    "dst_ds = None # it guaranttee shapefile is successfully created\n",
    "src_ds = None\n",
    "\n",
    "# STEP 6\n",
    "# post-processing\n",
    "infile = dst_layername + \".geojson\"\n",
    "outfile = os.path.join(working_dir, 'tile{}_{}_seg.geojson'.format(tile_id, season))\n",
    "command = 'Rscript'\n",
    "path_script = \"Postprocessing.R\"\n",
    "args = [infile, outfile, '0.00005']\n",
    "if (os.path.isfile(path_script) == False):\n",
    "    print('Fail to find Postprocessing.R')\n",
    "# check_output will run the command and store to result\n",
    "cmd = [command, path_script] + args\n",
    "x = subprocess.check_output(cmd, universal_newlines=True)\n",
    "\n",
    "# remove temporal file\n",
    "if verbose is False:\n",
    "    outpath = os.path.join(working_dir, 'tile{}_{}_watershed_overlap_merge.tif'.format(tile_id, season))\n",
    "    os.remove(outpath)\n",
    "    outpath = os.path.join(working_dir, 'tile{}_{}_watershed_overlap_merge_sieve.tif'.format(tile_id, season))\n",
    "    os.remove(outpath)\n",
    "    outpath = os.path.join(working_dir, 'tile{}_{}_watershed_merge_sieve_overlap.geojson'.format(tile_id, season))\n",
    "    os.remove(outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = dst_layername + \".geojson\"\n",
    "outfile = os.path.join(working_dir, 'tile{}_{}_seg.geojson'.format(tile_id, season))\n",
    "command = 'Rscript'\n",
    "path_script = \"Postprocessing.R\"\n",
    "args = [infile, outfile, '0.00005']\n",
    "if (os.path.isfile(path_script) == False):\n",
    "    print('Fail to find Postprocessing.R')\n",
    "# check_output will run the command and store to result\n",
    "cmd = [command, path_script] + args\n",
    "x = subprocess.check_output(cmd, universal_newlines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48, 42, 38, ..., 67, 63, 59],\n",
       "       [51, 45, 37, ..., 63, 61, 57],\n",
       "       [52, 47, 37, ..., 60, 58, 48],\n",
       "       ...,\n",
       "       [33, 27, 26, ..., 10, 25, 27],\n",
       "       [34, 29, 28, ..., 11, 28, 26],\n",
       "       [30, 31, 27, ..., 29, 26, 29]], dtype=int8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_prob\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_python3",
   "language": "python",
   "name": "cv_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
